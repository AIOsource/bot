import json
import logging
from typing import Optional, List, Dict
from datetime import datetime
from openai import OpenAI
from models import NewsArticle, FilteredEvent
from config import config
from database import db

logger = logging.getLogger(__name__)


class AIFilter:
    def __init__(self):
        self.client = OpenAI(api_key=config.PERPLEXITY_API_KEY, base_url=config.PERPLEXITY_API_BASE)
        self.model = config.PERPLEXITY_MODEL
        self.threshold = config.LLM_RELEVANCE_THRESHOLD
        
        # Stage 2 Configs
        self.positive_keywords = config.KEYWORDS_POSITIVE
        self.negative_keywords = config.KEYWORDS_NEGATIVE
        self.weights = config.SCORE_WEIGHTS
        self.score_threshold = config.KEYWORD_SCORE_THRESHOLD
    
    def filter_article(self, article: NewsArticle) -> Optional[FilteredEvent]:
        try:
            # 1. Pre-filtering (Weighted Scoring)
            keyword_score = self._calculate_keyword_score(article.title + " " + article.content)
            
            if keyword_score < self.score_threshold:
                # logger.debug(f"Skipping {article.title[:30]}... (Score: {keyword_score})")
                return None
                
            logger.info(f"üîé Analyzing (Score {keyword_score}): {article.title[:50]}...")

            # 2. LLM Analysis
            prompt = self._create_analysis_prompt(article)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤ –ñ–ö–•. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—ã—Ç–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            result = self._parse_ai_response(response)
            
            if not result:
                logger.warning(f"Failed to parse AI response for: {article.title}")
                return None
            
            # 3. Post-processing & Validation
            relevance = result.get('relevance', 0.0)
            urgency = result.get('urgency', 1)
            
            if relevance < self.threshold:
                logger.info(f"  üí§ REJECTED (Relevance {relevance:.2f} < {self.threshold})")
                return None
                
            logger.info(f"  ‚úÖ ACCEPTED (Relevance {relevance:.2f} | Urgency {urgency})")
            
            event = FilteredEvent(
                article_id=article.id,
                title=article.title,
                url=article.url,
                relevance_score=relevance,
                category=result.get('event_type', 'other'),
                urgency=urgency,
                object=result.get('object', 'unknown'),
                why=result.get('why', 'No explanation'),
                action=result.get('action', 'ignore'),
                filtered_at=datetime.now()
            )
            
            # Save raw 'why' as reasoning for backward compatibility if needed, or just use 'why'
            event.reasoning = f"{event.why} (Action: {event.action})"
            
            event_dict = event.model_dump()
            event_dict['filtered_at'] = event_dict['filtered_at'].isoformat()
            db.save_filtered_event(event_dict)
            
            return event
            
        except Exception as e:
            logger.error(f"Error filtering article {article.title}: {e}")
            return None

    def _calculate_keyword_score(self, text: str) -> int:
        score = 0
        text_lower = text.lower()
        
        # Positive weights
        for word in self.positive_keywords:
            if word in text_lower:
                # Basic logic: if keyword found, add points based on category
                # For simplicity, we'll try to map keywords to categories or just use a default positive weight
                if word in ["–∞–≤–∞—Ä–∏—è", "–ø—Ä–æ—Ä—ã–≤", "–æ—Å—Ç–∞–Ω–æ–≤–∫–∞"]:
                    score += self.weights.get("accident", 3)
                elif word in ["—Ä–µ–º–æ–Ω—Ç", "–∑–∞–º–µ–Ω–∞"]:
                    score += self.weights.get("repair", 2)
                elif word in ["–≤–æ–¥–æ–∫–∞–Ω–∞–ª", "–∫–æ—Ç–µ–ª—å–Ω–∞—è", "–Ω–∞—Å–æ—Å–Ω–∞—è"]:
                    score += self.weights.get("infra", 4)
                elif word in ["—Ü–µ—Ö", "–∞–≥—Ä–µ–≥–∞—Ç"]:
                    score += self.weights.get("industry", 2)
                else:
                    score += 1 # Default positive
        
        # Negative weights
        for word in self.negative_keywords:
            if word in text_lower:
                score += self.weights.get("negative", -5)
        
        return score
    
    def _create_analysis_prompt(self, article: NewsArticle) -> str:
        return f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç—å –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –µ—ë –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤ –ñ–ö–•.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}
–¢–µ–∫—Å—Ç: {article.content[:2000]}

–ü–†–ê–í–ò–õ–ê:
1. –¢–∏–ø —Å–æ–±—ã—Ç–∏—è (event_type):
   - accident: –∞–≤–∞—Ä–∏—è, –ø—Ä–æ—Ä—ã–≤, —É—Ç–µ—á–∫–∞, –ø–æ–ª–æ–º–∫–∞, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞, –≤—ã—Ö–æ–¥ –∏–∑ —Å—Ç—Ä–æ—è
   - outage: –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Å–≤–µ—Ç–∞/–≤–æ–¥—ã/—Ç–µ–ø–ª–∞ (–±–µ–∑ —è–≤–Ω–æ–π –∞–≤–∞—Ä–∏–∏)
   - repair: —Ä–µ–º–æ–Ω—Ç, –∑–∞–º–µ–Ω–∞, –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏—è, —Ä–∞–±–æ—Ç—ã
   - other: —É—á–µ–Ω–∏—è, –î–¢–ü, –ø–æ–∂–∞—Ä—ã (–Ω–µ –ñ–ö–•), –∫—Ä–∏–º–∏–Ω–∞–ª, –ø—Ä–æ—á–µ–µ

2. –°—Ñ–µ—Ä–∞ (object):
   - water: –≤–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ, –∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è, –Ω–∞—Å–æ—Å—ã
   - heat: –æ—Ç–æ–ø–ª–µ–Ω–∏–µ, –∫–æ—Ç–µ–ª—å–Ω—ã–µ, —Ç–µ–ø–ª–æ—Å–µ—Ç–∏
   - industrial: –∑–∞–≤–æ–¥—ã, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –∞–≥—Ä–µ–≥–∞—Ç—ã
   - unknown: –Ω–µ —è—Å–Ω–æ

3. –°—Ä–æ—á–Ω–æ—Å—Ç—å (urgency 1-5):
   - 1: –ü–ª–∞–Ω–æ–≤—ã–µ, –Ω–µ–≤–∞–∂–Ω—ã–µ
   - 3: –í–∞–∂–Ω—ã–µ (–∏–¥—É—Ç —Ä–∞–±–æ—Ç—ã, –æ—Ç–∫–ª—é—á–µ–Ω–∏—è)
   - 5: –ß–ü, —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ, –º–∞—Å—Å–æ–≤—ã–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è

4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (relevance 0.0-1.0):
   - 0.8-1.0: –í—ã—Å–æ–∫–∞—è (–ê–≤–∞—Ä–∏–∏, —Ä–µ–∞–ª—å–Ω—ã–µ –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã)
   - 0.6-0.7: –°—Ä–µ–¥–Ω—è—è (–†–µ–º–æ–Ω—Ç—ã, –æ—Ç–∫–ª—é—á–µ–Ω–∏—è)
   - <0.6: –ù–∏–∑–∫–∞—è (–ú—É—Å–æ—Ä, –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ç–µ–º–µ)

5. –î–µ–π—Å—Ç–≤–∏–µ (action):
   - call: –ï—Å–ª–∏ relevance >= 0.6 –ò urgency >= 3
   - watch: –ï—Å–ª–∏ relevance >= 0.6 –ò urgency < 3
   - ignore: –ò–Ω–∞—á–µ

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
  "event_type": "accident|outage|repair|other",
  "relevance": float,
  "urgency": int,
  "object": "water|heat|industrial|unknown",
  "why": "–û–¥–Ω–∞ —Ñ—Ä–∞–∑–∞ - –ø—Ä–∏—á–∏–Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏",
  "action": "call|watch|ignore"
}}
"""
    
    def _parse_ai_response(self, response) -> Optional[Dict]:
        try:
            content = response.choices[0].message.content.strip()
            # Clean markdown
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
                
            result = json.loads(content)
            
            # Validate essential fields
            required = ['event_type', 'relevance', 'urgency']
            if not all(k in result for k in required):
                logger.warning(f"AI response missing fields: {result.keys()}")
                return None
                
            result['relevance'] = float(result['relevance'])
            return result
        except Exception as e:
            logger.error(f"Error parsing AI response: {e}")
            return None


ai_filter = AIFilter()
